<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>David Bau</title>
<meta name="description" content="David Bau is a PhD student in Antonio Torralba's lab who develops techniques that increase interpretability of deep neural networks.">
<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<style>
body, td, th {
  font-family: 'Open Sans', sans-serif;
  background-color: #e4f1fe;
}
.prologue {
  margin-bottom: 1rem;
}
span.me {
  font-weight: bold;
  white-space: nowrap;
}
h1, h2, h3, h4, h5, h6 {
  font-weight: 300;
  text-align: center;
}
input.collapse {
  display: none;
}
h2, .namecard {
  background-color: #0E2863;
  color: #e4f1fe;
  position: relative;
  padding: 30px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24);
}
.collapse + p, .prologue {
  background-color: white;
  position: relative;
  padding: 30px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.12), 0 1px 2px rgba(0,0,0,0.24);
  transition: all 0.3s cubic-bezier(.25,.8,.25,1);
}
.collapse + p:hover {
  box-shadow: 0 14px 28px rgba(0,0,0,0.25), 0 10px 10px rgba(0,0,0,0.22);
  cursor: default;
}
.collapse + p .more::before {
  content: 'more\25B9';
  padding-right: .2em;
  color: #0275d8;
  cursor: pointer;
  transition: font-size 0.2s;
}
.collapse + p:hover .more::before {
  color: seagreen;
}
.collapse:checked + p .more::before {
  content: '\25C3';
  color: lightgray;
  position: absolute;
  font-style: normal;
  right: 3px;
  bottom: 0;
}
.collapse + p .more:hover::before {
  color: lightseagreen;
}
.collapse + p .extra {
  font-size: 0;
  opacity: 0;
  transition: opacity 0.5s, font-size 0.2s;
}
.collapse:checked + p .extra {
  font-size: 16px;
  opacity: 1;
}
.collapse + p::after {
  content: '';
  display: block;
  clear: both;
  padding: 0;
  margin: 0;
}
.footer {
  margin-top: 8px;
  text-align: justify;
  /*word-spacing: 10px; */
  width: 100%;
}
.footer::after {
  content: '';
  width: 100%;
  display: inline-block;
}
.footer a {
  word-spacing: initial;
  display: inline-block;
}
.namecard {
  padding: 2rem 15px 1rem;
  text-align: center;
  font-weight: 300;
}
.namecard a {
  color: inherit;
}
.namecard h1 {
  font-size: 2.5rem;
  font-weight: 300;
  color: inherit;
}
.selfpic {
  width: 448px;
  max-width: 100%;
  display: block;
}
.selfpic iframe {
  max-width: 100%;
}
.projpic {
  width: 288px;
  max-width: 100%;
  display: block;
  margin: 0 auto 15px auto;
}
figcaption {
  display: block;
  text-align: center;
  font-size: 12px;
  margin-top: 3px;
}
.highlight {
  padding: 1.5rem;
  margin-right: 0;
  margin-left: 0;
  background: gainsboro;
}
@media (min-width: 576px) {
  .nd-pageheader {
    padding-top:1rem;
    padding-bottom: 1rem;
  }
}
@media (min-width: 768px) {
  .nd-pageheader h1 {
    font-size:3rem
  }
  .nd-pageheader p {
    font-size: 1.5rem
  }
  .selfpic {
    display: inline-block;
  }
  .projpic {
    float: left;
    margin: 0 1rem 0 0;
  }
}
</style>
</head>
<body class="db-page">
<div class="container">
<div class="row">
<div class="col">
 <div class="namecard">
 <h1><nobr>David Bau</nobr></h1>
 <p class="lead">
 <nobr>Creating Transparency</nobr> in a
 <nobr>Complex World</nobr>
<address>
 <a href="https://www.csail.mit.edu/about">
 <nobr>MIT Computer Science</nobr> and
 <nobr>Artificial Intelligence Laboratory</nobr></a><br>
 <!-- <nobr>Massachusetts Institute of Technology</nobr><br> -->
 <nobr><script>
var emn = 'davidbau' + ''.constructor.fromCharCode(1<<6) + 'csail.mit.edu';
document.write('<a href="mailto:' + emn + '">' + emn + '</a>');
 </script></nobr><br>
</address>
 </p>
 </div>
<div class="prologue clearfix">
<figure class="float-right ml-3 selfpic">
  <!--<img src="family.jpg" class="selfpic">
  <figcaption>David Bau (center) and family:<br>(clockwise) Heidi, Cody, Piper, and Anthony</figcaption>-->
<iframe id="selfpic-video" width="448" height="252" src="https://www.youtube.com/embed/jT5nYLND7co?controls=1&playsinline=1&loop=1&modestbranding=1&rel=0&cc_load_policy=1&enablejsapi=1&origin=https://people.csail.mit.edu" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<figcaption>Why we study deep network internals.  <nobr>David Bau</nobr> (narrates), with Antonio Torralba, Jun-Yan Zhu, Hendrik Strobelt, Jonas Wulff, and William Peebles. <nobr>Video by Lillie Paquette,</nobr> MIT School of Engineering.</figcaption>
</figure>
<p>It is crucial for humans to understand the algorithms
discovered by machine-learning systems so we can explain,
design, and control their behavior in new domains.
<p>Artificial intelligence should create transparency in a complex world.
As our sophisticated systems exhibit more unexpected and emergent
behavior, I believe the focus of machine learning should shift to
<em>machine explanation</em>: the development of methods that reveal
the patterns and strategies our algorithms find, so that
programmers and users can retain agency.
</p>
<p>As a PhD student in <a href="http://web.mit.edu/torralba/www/"
>Antonio Torralba's lab</a>, I develop techniques that increase
interpretability
of deep neural networks. I have found evidence that the
representations learned by deep networks can have a
simple underlying structure.
The focus of my research at MIT is therefore to
find ways to enhance and exploit this structure
to make deep networks more transparent.</p>
<p>I also have a long interest on the wider problem of making all
computer systems more programmable. I develop programming
interfaces, open standards, tools and libraries that make complex
systems easier to understand.
</div> <!-- prologue -->
</div>
</div>
<div class="row">
<div class="col">
<div class="publication">
<h2 class="mt-4 mb-3">Selected Publications and Projects</h2>
<input class="collapse" type="checkbox">
<p>
<img src="seeing.png" class="projpic">
<a href="http://gandissect.csail.mit.edu/papers/Seeing_What_a_GAN_Cannot_Generate.pdf">Seeing what a GAN Cannot Generate</a>
studies mode dropping by asking the inverse question:
how can we decompose and understand what a GAN <em>cannot</em> do?
<span class="more"></span>
<span class="extra">A core challenge faced by GANs is <em>mode dropping</em>
or <em>mode collapse</em>, which is the tendendency for a GAN generator to
focus on a few modes and omit other parts of the distribution.
State-of-the-art GANs apply training methods designed to reduce
mode collapse, but analyzing the phenomenon remains difficult for large
distributions: examination of output samples reveals what a GAN <em>can</em>
do, not what it <em>cannot</em> do.  So in this paper we
develop a pair of complementary methods for decomposing what GAN omits,
looking at segmentation statistics over a distribution, and also visualizing
omissions in specific instances by calculating inversions of a GAN generator.
Surprisingly, we find that a state-of-the-art GAN will sometimes cleanly
omit whole classes of objects from its output, hiding these omissions
by creating realistic instances without those objects.
</span>
<a href="http://gandissect.csail.mit.edu/papers/Seeing_What_a_GAN_Cannot_Generate.pdf"
><span class="me">D Bau</span>, JY Zhu, J Wulff, W Peebles, H Strobelt,
B Zhou, A Torralba. <em>Seeing What a GAN Cannot Generate</em>.
ICCV 2019 oral</a>.
</p>
<input class="collapse" type="checkbox">
<p>
<img src="parliament.gif" class="projpic">
<a href="http://ganpaint.io/">
GAN Paint</a>
applies <a href="http://gandissect.csail.mit.edu/">
GAN dissection</a> to the manipulation of user-provided real
photographs.  By encoding a scene into a representation that can
be rendered by a generator network derived from a GAN, a user
can manipulate photo semantics, painting objects such as doors,
windows, trees,
and domes.  The details of rendering objects in plausible
configurations is left to the network.
<span class="more"></span>
<span class="extra">
Our previous GAN dissection work showed how to manipulate random
synthetic images generated by an unconditional GAN.  To manipulate
a real photograph <em>X</em> instead,
the generator must be guided to reproduce the photograph faithfully.
While <a href="https://github.com/junyanz/iGAN">previous work</a>
has investigated finding the best input <em>z</em>
so that <em>G(z)&asymp;X</em>, we show that it is useful to also optimize
the parameters of <em>G</em> itself.  Even in cases where the GAN is not
capable of rendering the details of the user-provided photo, a nearby
GAN generator can be found that does. We
implemented our algorithm using an interactive painting app at
<a href="http://ganpaint.io/">ganpaint.io</a>.
</span>
<a href="https://dl.acm.org/citation.cfm?id=3323023"
><span class="me">D Bau</span>, H Strobelt, W Peebles, J Wulff, B Zhou,
JY Zhu, A Torralba. <em>Semantic Photo Manipulation
  with a Generative Image Prior</em>.
  In SIGGRAPH 2019</a>.
</p>
<input class="collapse" type="checkbox">
<input class="collapse" type="checkbox">
<p>
<img src="gandissect.png" class="projpic">
<a href="http://gandissect.csail.mit.edu/">
GAN Dissection</a>
investigates the internals of a GAN, and shows how neurons can be
<a href="http://gandissect.res.ibm.com/ganpaint.html">directly manipulated</a>
to change the behavior of a generator.
<span class="more"></span>
<span class="extra">
Here we ask whether the apparent
structure that we found in classifiers also appears in a setting with
no supervision from labels.  Strikingly, we find that a state-of-the-art
GAN trained to generate complex scenes will learn neurons that are
specific to types of objects in the scene, such as neurons for trees,
doors, windows, and rooftops.
The work shows how to find such neurons, and shows that by forcing
the neurons on and off, you can cause a generator to draw or remove
specific types of objects in a scene.
</span>
<a href="https://openreview.net/pdf?id=Hyg_X2C5FX"
><span class="me">D Bau</span>, JY Zhu, H Strobelt, B Zhou, JB Tenenbaum,
WT Freeman, A Torralba. <em>GAN Dissection: 
  Visualizing and Understanding Generative Adversarial Networks</em>.
  In ICLR 2019</a>.
</p>
<input class="collapse" type="checkbox">
<p>
<img src="netdissect.png" class="projpic">
<a href="http://netdissect.csail.mit.edu/">
Network Dissection</a>
is a technique for quantifying and automatically
estimating the human interpretability (and interpretation) of units
within any deep neural network for vision.
<span class="more"></span>
<span class="extra">
Building upon a surprising
<a href="https://arxiv.org/abs/1412.6856">2014 finding by Bolei Zhou</a>,
network dissection defines a dictionary of 1197 human-labeled
visual concepts, each represented as a segmentation problem, then
it estimates interpretability by evaluating each hidden convolutional
unit as a solution to those problems.
I have used network dissection to reveal that representation space is
not isotropic: learned representations have an unusually high agreement
with human-labeled concepts that vanishes under a change in basis.
We gave an oral presentation about
the technique and the insights it provides at CVPR 2017.
</span>
<a href="http://netdissect.csail.mit.edu/final-network-dissection.pdf"
><span class="me">D Bau</span>, B Zhou, A Khosla, A Oliva, and A Torralba. <em>Network Dissection:
Quantifying the Intepretability of Deep Visual Representations.</em>
CVPR 2017 oral</a>.
</p>
<input class="collapse" type="checkbox">
<p>
<img src="bbchunking.png" class="projpic">
<a href="http://cs.wellesley.edu/~blocks-and-beyond/1/">
Blocks and Beyond</a>
is a workshop I helped organize to bring together researchers who are
investigating blocked-based interfaces to simplify programming for
novices and casual programmers.
<span class="more"></span>
<span class="extra"> The workshop was oversubscribed, and the presented
work was interesting both for its breadth and depth.  Afterwards, we
wrote a review paper to survey the history, foundations, and
state-of-the-art in the field.
The review appears in the June 2017 Communications of the ACM; <a href="https://vimeo.com/216045469">also see the video overview</a>.</span>
<a href="http://dl.acm.org/authorize?N38021" title="Learnable programming: blocks and beyond"><span class="me">D Bau</span>, J Gray, C Kelleher, J Sheldon, F Turbak. <em>Learnable Programming: Blocks and Beyond.</em> Communications of the ACM, pp. 72-80. June 2017</a>.</p>
<input class="collapse" type="checkbox">
<p>
<img src="pencilgym.png" class="projpic">
<a href="https://pencilcode.net">
Pencil Code</a> is an open-source
project that makes it easier for novice programmers to work with
professional programming languages.
<span class="more"></span>
<span class="extra">Developed together with my son and with the generous
support of Google, this system provides a blocks-based editing environment
with turtle graphics on a canvas that smoothly transitions to text-based
editing of web applications using jQuery. Two thousand students use the
system each day. A study of middle-school
students using the environment suggests suggests the block-and-text
transitions are an aid to learning.</span>
<a href="http://dl.acm.org/authorize?N38022"><span class="me">D Bau</span>, D A Bau, M Dawson, C S Pickens. <em>Pencil code: block code for a text world.</em> In Proceedings of the 14th International Conference on Interaction Design and Children, pp. 445-448. ACM, 2015.</a>
</p>
<input class="collapse" type="checkbox">
<p>
<img src="freshimage.png" class="projpic">
<a href="https://images.google.com/">
Google Image Search</a> is the world's largest searchable index of images.
<span class="more"></span>
<span class="extra">I contributed several improvements to this product, including improved <a href="https://search.googleblog.com/2011/12/search-quality-highlights-new-monthly.html">ranking for recent images</a>, a <a href="https://googlesystem.blogspot.com/2013/06/new-ui-for-related-searches-in-google.html#gsc.tab=0">clustered broswing interface for discovering images</a> using related searches, a rollout of new serving infrastructure to support <a href="https://googleblog.blogspot.com/2010/07/ooh-ahh-google-images-presents-nicer.html">a long-scrolling result page</a>
 serving one thousand image results at a time, and improvements in the understanding of person entities on the web.</span>
<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.722.1005">M Zhao, J Yagnik, H Adam, <span class="me">D Bau</span>, Large scale learning and recognition of faces in web videos. In Automatic Face & Gesture Recognition, 2008. FG'08. 8th IEEE International Conference on (pp. 1-7). IEEE, September 2008.</a>
</p>
<input class="collapse" type="checkbox">
<p>
<img src="gmailchat.png" class="projpic">
<a href="https://en.wikipedia.org/wiki/Google_Talk">
Google Talk</a>
is a web-based chat solution that was built-in to GMail.
<span class="more"></span>
<span class="extra">I led the team to create Google Talk in an (ultimately unsuccessful) attempt to establish a universal federated open realtime communication ecosystem for the internet.  Our messaging platform provided full-scale support for XMPP and Jingle, which are open standards for federating real-time chat and voice that are analogous to the open-for-all SMTP system for email. When these open protocols came under asymmetric attack by Microsoft (they provided only one-way compatibility), Google relented and reverted to a closed network. To this day, open realtime communications remains an unfulfilled dream for the internet.</span>
<a href="https://googleblog.blogspot.com/2005/08/google-gets-to-talking.html"><span class="me">D Bau</span>. <em>Google Gets to Talking</em>. Google Official Blog, August 2005.</a>
</p>
<input class="collapse" type="checkbox">
<p>
<img src="xmlbeans.png" class="projpic">
<a href="https://en.wikipedia.org/wiki/XMLBeans">
Apache XML Beans</a> is an open-source implementation of the XML Schema specification as a compiler from schema types to Java classes.
<span class="more"></span>
<span class="extra">While no longer widely used, my team's implementation of this standard is still a good example of an important approach that continues to be a key technique for the creation of understandably complex systems: the prioritization of faithful and transparent data representations over simplified but opaque functional encapsulations.</span>
<a href="http://davidbau.com/archives/2003/11/14/the_design_of_xmlbeans_part_1.html"><span class="me">D Bau</span>. <em>The Design of XML Beans</em>, davidbau.com, a dabbler's weblog, November 2003.</a>
</p>
<input class="collapse" type="checkbox">
<p>
<img src="ie4logo.png" class="projpic">
<a href="https://en.wikipedia.org/wiki/Internet_Explorer_4">
Microsoft Internet Explorer 4</a> was the first AJAX web browser.
<span class="more"></span>
<span class="extra">As part of the <a href="https://en.wikipedia.org/wiki/Trident_(layout_engine)">Trident</a> team led by <a href="https://en.wikipedia.org/wiki/Adam_Bosworth">Adam Bosworth</a>, I helped create the first fully mutable HTML DOM by defining its asynchronous loading model. My contribution was to implement an incremental HTML parser that uses speculative lookahead to drive a fast multithreaded preloader for linked resources, while maintaining a consistent view of programmable elements for single-threaded scripts that can change the document during loading.</span>
The design of the system resolved tensions between performance, flexiblity, and programmability, and contributed to the strength of the modern web platform.</p>
<input class="collapse" type="checkbox">
<p>
<img src="linalg.png" class="projpic">
<a href="http://people.maths.ox.ac.uk/~trefethen/text.html">
Numerical Linear Algebra</a> is the graduate textbook on numerical linear algebra I wrote with my advisor <a href="http://people.maths.ox.ac.uk/~trefethen/">Nick Trefethen</a> while earning a Masters at Cornell.
<span class="more"></span>
<span class="extra">The book began as a detailed set of notes that I took while attending Nick's course. The writing is intended to capture the spirit of his teaching: succinct and insightful.  The hope is to reveal the elegance of this family of fundamental algorithms and dispel the myth that finite-precision arithmetic means imprecise thinking.</span>
<a href="https://scholar.google.com/scholar?cluster=4058884683459283601">L N Trefethen, <span class="me">D Bau</span>. Numerical linear algebra. Vol. 50. Siam, 1997.</a></p>
</div> <!-- publication -->
</div>
</div>
<div class="row">
<div class="col">
<div class="footer">
        <a href="http://dblp.uni-trier.de/pers/hd/b/Bau:David">Dblp</a>
        <a href="https://scholar.google.com/citations?hl=en&user=CYI6cKgAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a>
        <a href="https://github.com/davidbau">Github</a>
        <a href="https://www.linkedin.com/in/david-bau-4b8130/">LinkedIn</a>
        <a href="http://davidbau.com">Personal Blog</a>
        <a href='https://www.google.com/search?safe=off&noj=1&biw=1855&bih=917&q=david+bau+-music+-mp3+-radio+-youtube+-soundcloud+-minnesota+-tennis+-"records"+-"noah+david"+-"extension+educator"+-"bau+mann"+-"bau+man"+-"bau+telle"+-"bau+xi"+-mirador+-atticus&oq=david+bau+-music+-mp3+-radio+-youtube+-soundcloud+-minnesota+-tennis+-"records"+-"noah+david"+-"extension+educator"+-"bau+mann"+-"bau+man"+-"bau+telle"+-"bau+xi"+-mirador+-atticus'>Web</a>
        <a href="https://whereis.mit.edu/?zoom=16&lat=42.361612839999985&lng=-71.09056785999995&maptype=mit&q=32-383&open=object-32">Office: Stata 32-383</a>
        </div>
        </div>
        </div>
        </div>
        <script>
        $(document).on('click', '.more, .publication p', function(ev) {
          // Do not collapse a card if the click is for a hyperlink or selection.
          if ($(ev.target).closest('a').length ||
                  getSelection().type == 'Range' || ev.shiftKey || ev.ctrlKey) {
            return;
          }
          // Use the hidden checkbox technique to preserve collapse-state on "back"
          var collapse = $(this).closest('p').prev('input.collapse');
          collapse.prop('checked', !collapse.prop('checked'));
          ev.stopPropagation();
        });
        </script>
        </body>
<script type="text/javascript">
  var player;
  function onYouTubeIframeAPIReady() {
    player = new YT.Player('selfpic-video', { events: {
     'onStateChange': function() {
        player.setOption('captions', 'fontSize', 2);
      },
    }});
  }
</script>
<script src="https://www.youtube.com/iframe_api"></script>
</html>
